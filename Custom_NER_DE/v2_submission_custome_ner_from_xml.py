# -*- coding: utf-8 -*-
"""V2_Submission_Custome_NER_From_XML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fo58HIFehKZc7c_kAONx-vdL10uXpi1t

##**Part 1 :: Creating training labels from XML file to train custom Spacy Model**
"""

!mkdir xml_files #create a directory for interacting with operating system

pip install -U spacy

#Load the .zip into your directory and rename he path in the cell below

!unzip Protokoll-Zionistenkongress-Basel_1897-0200.zip -d xml_files/ #unzips the zip file which contains all .xml files with the annotations of the Persons and Locations which were pre-defined

#Import these packages and dependencies - have also a look at "requirements.txt"

import os
import xml.etree.ElementTree as ET

xml_files = os.listdir("xml_files")

xml_files = sorted(xml_files)

len(xml_files)

"""**Below cell is the logic for the parsing XML file and extracting label and converting it to spacy format.**"""

# add words in the list to remove beforehand
word_remove = ["HÃ¤ndeklatschen"]

final_all_ents_tuple = []
all_sentences_present = []

#looping over all the files
for j in range(len(xml_files)):

  print("processing file=================================== ", xml_files[j])
  
  mytree = ET.parse('xml_files/'+xml_files[j])
  myroot = mytree.getroot()

  for x in myroot[1][1]:
    if x.tag.endswith('TextLine'):
      if "person" in x.attrib['custom'] or "place" in x.attrib['custom']:
 
        ents = x.attrib['custom'].split(" ")[2:]
        print(ents)
        sentence = x[-1][0].text
        all_sentences_present.append(sentence)
 
        all_ents = []
 
        for i in range(0, len(ents)):
          if ents[i] in ['person', 'place']:
            if ents[i] == 'person':
               ent = 'PERSON'
            else:
              ent = 'LOC'
 
            a = int(ents[i+1].split(":")[1][:-1])

            ## following if-else condition is written as there are some labels which has 'continued:true' means there are more word belong to current word
            if ents[i+2].endswith("}"):
              b = int(ents[i+2].split(":")[1][:-2])
            else:
              try:
                i += 4
                b1 = int(a[i+1].split(":")[1][:-1])
                if ents[i+2].endswith("}"):
                  b2 = int(ents[i+2].split(":")[1][:-2])
                else:
                  b2 = int(ents[i+2].split(":")[1][:-1])
                b = b1 + b2
              except:
                i -= 4
                b = int(ents[i+2].split(":")[1][:-1])
 
            # checking if word is part of words to remove
            if not list(set(sentence[a:a+b].split(" ")) & set(word_remove)):
              ent_tuple = [a, a+b, ent] #single tuple as per the format defined by spacy
              all_ents.append(ent_tuple)
            else:
              continue

        # following loop is written because in there are some samples which has overlapping range, this loop handles those overlapping words as they are already covered.
        all_ents_copy = all_ents.copy()
        for k in range(len(all_ents)-1):
          if all_ents[k][0] <= all_ents[k+1][0] <= all_ents[k][1] or all_ents[k][0] <= all_ents[k+1][1] <= all_ents[k][1]:
            try:
              del all_ents_copy[k+1]
            except:
              del all_ents_copy[k]

        if all_ents_copy:
          final_tuple = (sentence, {'entities' : all_ents_copy})
          print(final_tuple)
        final_all_ents_tuple.append(final_tuple)  #this variable holds all the tuples from all the files
        print("=="*50)

# storing all the labels in txt file called "outfile.txt"
with open("outfile.txt", "w") as outfile:
    outfile.write("\n".join(str(item) for item in final_all_ents_tuple))

"""##**Part 2 :: Custom training with Spacy**"""

!pip show spacy

#More packages to install
from __future__ import unicode_literals, print_function
import plac
import random
from pathlib import Path
import spacy
from tqdm import tqdm
from spacy.util import minibatch, compounding
from spacy.training import Example
from spacy.pipeline import EntityRuler

!mkdir de_spacy_custom_v2

model = None
output_dir=Path("/content/de_spacy_custom_v2") #output folder in which trained model will be stored
n_iter=100 #number of training epochs (increase for better performance or decrease for shorter run time - rule of thumb : minimum 40 epochs required)

if model is not None:
    nlp = spacy.load(model)  
    print("Loaded model '%s'" % model)
else:
    nlp = spacy.blank('de')  
    print("Created blank 'de' model")

#set up the pipeline

if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe('ner')
    nlp.add_pipe('ner')
else:
    ner = nlp.get_pipe('ner')

"""**Add words in the list to be considered in entity ruler**"""

# list of words to be added as person for training using entity ruler
person_names = ['Gustav Gottheil', 'Max Mustermann'] #add manually names here

person_patterns = []

for i in range(len(person_names)):
  person_patterns.append({"label": "PERSON", "pattern": person_names[i]})

# list of words to be added as location for training using entity ruler
location_names = ['Boston', 'New-York'] #add manually places/locations/cities here

location_patterns = []

for i in range(len(location_names)):
  location_patterns.append({"label": "LOC", "pattern": location_names[i]})

patterns = person_patterns + location_patterns

"""**Creating Entity Ruler with custom patterns**"""

cfg = {"overwrite_ents": True} #add an entitiy ruler for the manual changes
nlp.add_pipe('entity_ruler', before='ner', config=cfg).add_patterns(patterns)

"""**Below cell is for spacy training code**

**This can take up to 3 hours to complete the training for the 100 epochs.**
"""

for _, annotations in final_all_ents_tuple:
    for ent in annotations.get('entities'):
        ner.add_label(ent[2])

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']

optimizer = nlp.begin_training()
for itn in range(n_iter):
    random.shuffle(final_all_ents_tuple)
    losses = {}
    # batch up the examples using spaCy's minibatch
    batches = minibatch(final_all_ents_tuple, size=compounding(4.0, 32.0, 1.001))
    for batch in batches:
        texts, annotations = zip(*batch)
        example = []
        # Update the model with iterating each text
        for i in range(len(texts)):
            doc = nlp.make_doc(texts[i])
            example.append(Example.from_dict(doc, annotations[i]))
        
        # Update the model
        nlp.update(example, drop=0.5, losses=losses)
        print("Losses", losses)

#saving trained model in directory
if output_dir is not None:
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.to_disk(output_dir)
    print("Saved model to", output_dir)

#sample inference using trained model
for text, _ in final_all_ents_tuple[:5]:
    doc = nlp(text)
    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])

"""##**Part 3 :: Inference of Custom trained model on test data**"""

# add words in this list which needs to be removed.
word_remove = ['Händeklatschen'] #seperate words with commas (,)

import pandas as pd #read the text file ".txt" to test the model -> rename if necessary
test_df = pd.read_csv('03_Protokoll-Zionistenkongress-Basel_1899.txt', delimiter = "\n", header=None, names=["text"])

test_df.info()

test_df.head(30)

all_persons = []
all_locations = []

for jj in range(len(test_df)):
  doc = nlp(test_df['text'][jj])
  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON' and ent.text not in word_remove]
  locations = [ent.text for ent in doc.ents if ent.label_ == 'LOC' and ent.text not in word_remove]
  all_persons.append(persons)
  all_locations.append(locations)

test_df['v2_Custom-trained_Spacy_Person'] = pd.Series(all_persons)
test_df['v2_Custom-trained_Spacy_Location'] = pd.Series(all_locations)

test_df.head(30)

test_df.tail(30)

test_df.info()

test_df.head(40)

##**These are the results of the own trained model - saves as "v2_Custom_NER_inference_results.csv" - change directory and/or name if needed**

test_df.to_csv('v2_Custom_NER_inference_results.csv',index=False) #saving inference results of custom trained model

"""##**Part 4 :: Using pre-trained German spacy Large model to detect entity**"""

!python -m spacy download de_core_news_lg

import pandas as pd

nlp = spacy.load("de_core_news_lg") #loading the large pre-trained spacy model for german language

df = pd.read_csv("v2_Custom_NER_inference_results.csv") #loads the csv of custom trained results -> change here if you renamed this file earlier

df.info()

all_persons = []
all_locations = []

for jj in range(len(df)):
  doc = nlp(df['text'][jj])
  persons = [ent.text for ent in doc.ents if ent.label_ == 'PER']
  locations = [ent.text for ent in doc.ents if ent.label_ == 'LOC']
  all_persons.append(persons)
  all_locations.append(locations)

df['Pre-trained_Spacy_Person'] = pd.Series(all_persons)
df['Pre-trained_Spacy_Location'] = pd.Series(all_locations)

df.info()

df.to_csv("v2_Custom_NER_All_Inference_results.csv", index=False) #saving final results which has results of custom model and pre-trained spacy large model.

!zip -r de_spacy_custom_v2.zip de_spacy_custom_v2/

df.tail(50)