{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "V2_Submission_Custome_NER_From_XML.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Binder for custom-ner-de"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create training labels from XML files to train custom spaCy model\n",
    "\n",
    "Input URL to PAGE XML Zip file exported from Transkribus (for example a public link pointing to a file on SWITCHdrive: https://drive.switch.ch/index.php/s/FILE/download):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ZIP_URL = input()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input list of words to be removed from the list of entities (false positives):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "WORD_REMOVE = input()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract entities:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from custom_ner_de.extract import extract_entities\n",
    "extract_entities(zip_path=ZIP_URL,\n",
    "                 word_remove=WORD_REMOVE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Part 2 :: Custom training with Spacy**"
   ],
   "metadata": {
    "id": "TGIa3klQ0k_z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip show spacy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4Qm77KDE1c5",
    "outputId": "4298a7a6-e77c-4fba-f7bb-024082b4ec88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: spacy\n",
      "Version: 3.3.1\n",
      "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
      "Home-page: https://spacy.io\n",
      "Author: Explosion\n",
      "Author-email: contact@explosion.ai\n",
      "License: MIT\n",
      "Location: c:\\users\\hinder0000\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\n",
      "Requires: requests, srsly, preshed, setuptools, blis, jinja2, typer, numpy, spacy-legacy, wasabi, pydantic, cymem, murmurhash, tqdm, thinc, catalogue, pathy, langcodes, spacy-loggers, packaging\n",
      "Required-by: \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#More packages to install\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "from spacy.pipeline import EntityRuler"
   ],
   "metadata": {
    "id": "0anv2kzL0t4K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plac'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#More packages to install\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m unicode_literals, print_function\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplac\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'plac'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir de_spacy_custom_v2"
   ],
   "metadata": {
    "id": "0g2jEorG066N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = None\n",
    "output_dir=Path(\"/content/de_spacy_custom_v2\") #output folder in which trained model will be stored\n",
    "n_iter=100 #number of training epochs (increase for better performance or decrease for shorter run time - rule of thumb : minimum 40 epochs required)"
   ],
   "metadata": {
    "id": "z5Oc_A7j0uVB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('de')  \n",
    "    print(\"Created blank 'de' model\")\n",
    "\n",
    "#set up the pipeline\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner')\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhgI33m20zgh",
    "outputId": "2a2cbe21-86a6-4d4e-8a3b-bb298699bf8a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Add words in the list to be considered in entity ruler**"
   ],
   "metadata": {
    "id": "XiX9mzCWDgpB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# list of words to be added as person for training using entity ruler\n",
    "person_names = ['Gustav Gottheil', 'Max Mustermann'] #add manually names here\n",
    "\n",
    "person_patterns = []\n",
    "\n",
    "for i in range(len(person_names)):\n",
    "  person_patterns.append({\"label\": \"PERSON\", \"pattern\": person_names[i]})"
   ],
   "metadata": {
    "id": "6tZ3TBicCM4e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# list of words to be added as location for training using entity ruler\n",
    "location_names = ['Boston', 'New-York'] #add manually places/locations/cities here\n",
    "\n",
    "location_patterns = []\n",
    "\n",
    "for i in range(len(location_names)):\n",
    "  location_patterns.append({\"label\": \"LOC\", \"pattern\": location_names[i]})"
   ],
   "metadata": {
    "id": "7zYyqjl5DxAe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "patterns = person_patterns + location_patterns"
   ],
   "metadata": {
    "id": "ygONL0qQE7PV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating Entity Ruler with custom patterns**"
   ],
   "metadata": {
    "id": "yJ88A-zIEXoO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cfg = {\"overwrite_ents\": True} #add an entitiy ruler for the manual changes\n",
    "nlp.add_pipe('entity_ruler', before='ner', config=cfg).add_patterns(patterns)"
   ],
   "metadata": {
    "id": "nE2jUzV1qkO7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Below cell is for spacy training code**\n",
    "\n",
    "**This can take up to 3 hours to complete the training for the 100 epochs.**"
   ],
   "metadata": {
    "id": "msg9oVDY4BK-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for _, annotations in final_all_ents_tuple:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ],
   "metadata": {
    "id": "ikn3O_umJRFv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']"
   ],
   "metadata": {
    "id": "e7yymVo1Jo6U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(final_all_ents_tuple)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(final_all_ents_tuple, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        example = []\n",
    "        # Update the model with iterating each text\n",
    "        for i in range(len(texts)):\n",
    "            doc = nlp.make_doc(texts[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(example, drop=0.5, losses=losses)\n",
    "        print(\"Losses\", losses)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLDkzaVv0zc9",
    "outputId": "4f1cc4bb-3f8a-4cc2-8089-184a01855d26",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#saving trained model in directory\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BO3Q2K-_TuUh",
    "outputId": "32890737-d594-480c-de5a-a91f44c6d570",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#sample inference using trained model\n",
    "for text, _ in final_all_ents_tuple[:5]:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSdhJieR0zZ4",
    "outputId": "7d4e47ec-d3ed-4819-ff3c-c2f051c81da6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Part 3 :: Inference of Custom trained model on test data**"
   ],
   "metadata": {
    "id": "FB6kZonawdR4",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# add words in this list which needs to be removed.\n",
    "word_remove = ['Händeklatschen'] #seperate words with commas (,)"
   ],
   "metadata": {
    "id": "luqFn_L9W_71",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd #read the text file \".txt\" to test the model -> rename if necessary\n",
    "test_df = pd.read_csv('03_Protokoll-Zionistenkongress-Basel_1899.txt', delimiter = \"\\n\", header=None, names=[\"text\"])"
   ],
   "metadata": {
    "id": "bMNQu94qwjSg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNeuPrm9wjPM",
    "outputId": "004b81c7-e2a7-49f8-deab-1d8936f70a42",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head(30)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "h2w2w0cHCANl",
    "outputId": "e46084fd-b3c3-4a01-e28e-c5ab49a06a21",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_persons = []\n",
    "all_locations = []\n",
    "\n",
    "for jj in range(len(test_df)):\n",
    "  doc = nlp(test_df['text'][jj])\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON' and ent.text not in word_remove]\n",
    "  locations = [ent.text for ent in doc.ents if ent.label_ == 'LOC' and ent.text not in word_remove]\n",
    "  all_persons.append(persons)\n",
    "  all_locations.append(locations)"
   ],
   "metadata": {
    "id": "EuA4As3zxfIP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5bcba25b-44db-4c24-9101-d0e887821a37",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df['v2_Custom-trained_Spacy_Person'] = pd.Series(all_persons)\n",
    "test_df['v2_Custom-trained_Spacy_Location'] = pd.Series(all_locations)"
   ],
   "metadata": {
    "id": "XIeMMxtHxxEp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head(30)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "rimW5XYZCFKT",
    "outputId": "25f31c12-59e9-48ca-ab01-d643994ada26",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.tail(30)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "81pjx5RWCGsF",
    "outputId": "ffab8f1a-edf2-4aa3-8c2a-f443ce9d353f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9pAHSdpxxBL",
    "outputId": "1016e46e-c59f-4692-ac57-496a6f0aa2fc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.head(40)"
   ],
   "metadata": {
    "id": "f5jQF-Bfxw-o",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "01f6995f-2184-4c6d-ee85-980b9ca5a08a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfPdbmWu8vKd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##**These are the results of the own trained model - saves as \"v2_Custom_NER_inference_results.csv\" - change directory and/or name if needed**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_df.to_csv('v2_Custom_NER_inference_results.csv',index=False) #saving inference results of custom trained model"
   ],
   "metadata": {
    "id": "mbZAzIjLxw5L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Part 4 :: Using pre-trained German spacy Large model to detect entity**"
   ],
   "metadata": {
    "id": "XF1Ffp64u2Mb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m spacy download de_core_news_lg"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQxWWHJTu73B",
    "outputId": "39659c15-64bb-4760-da16-08e788d27def",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "id": "JhG8Z5SA0A6t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\") #loading the large pre-trained spacy model for german language"
   ],
   "metadata": {
    "id": "XN2JEVUFvoA7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"v2_Custom_NER_inference_results.csv\") #loads the csv of custom trained results -> change here if you renamed this file earlier"
   ],
   "metadata": {
    "id": "rzeC6nPuz7I0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opQq0Rfdj-X3",
    "outputId": "f3bc4b5a-98a4-40af-9f07-2adba5ce6e7e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "all_persons = []\n",
    "all_locations = []\n",
    "\n",
    "for jj in range(len(df)):\n",
    "  doc = nlp(df['text'][jj])\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PER']\n",
    "  locations = [ent.text for ent in doc.ents if ent.label_ == 'LOC']\n",
    "  all_persons.append(persons)\n",
    "  all_locations.append(locations)"
   ],
   "metadata": {
    "id": "zVFTGmhJvH6B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Pre-trained_Spacy_Person'] = pd.Series(all_persons)\n",
    "df['Pre-trained_Spacy_Location'] = pd.Series(all_locations)"
   ],
   "metadata": {
    "id": "7_XxQftIzvA_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyGYGJbO0EaX",
    "outputId": "8d0897f6-6b3e-4381-f6d9-4cbfbf0039f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.to_csv(\"v2_Custom_NER_All_Inference_results.csv\", index=False) #saving final results which has results of custom model and pre-trained spacy large model."
   ],
   "metadata": {
    "id": "O6c7369s0I7V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r de_spacy_custom_v2.zip de_spacy_custom_v2/"
   ],
   "metadata": {
    "id": "ulLWHlcm64Ah",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd5752b1-8e04-468d-a9fc-37cce4a793bb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.tail(50)"
   ],
   "metadata": {
    "id": "Zx-VR0dOnvmu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "4a8bb20e-4dfa-46f2-f623-bd1cc38b75a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "iSJDvTEgniDb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Part 5 :: Calculating Accuracy score of Custom model and Spacy large model**"
   ],
   "metadata": {
    "id": "0JWxcrfRiHZZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "7cacxMSvnemY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to run below code please make sure below files are in same folder as notebook, if not, change path accordingly.\n",
    "\n",
    "- This Notebook\n",
    "- extracted_entities.txt\n",
    "- custom trained model folder"
   ],
   "metadata": {
    "id": "73Bpr_goigqC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -lh"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0TtTKJUi9S9",
    "outputId": "c1b90636-f5da-4d53-ea7f-37b6ab578f0b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from spacy.training import Example\n",
    "import spacy"
   ],
   "metadata": {
    "id": "HKW1wxZNiT-P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    function to load entity data\n",
    "\n",
    "    input ::\n",
    "        \n",
    "    output ::\n",
    "        - Entity data to use for accuracy calculation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"loading data...\")\n",
    "    file1=open('extracted_entities.txt')\n",
    "\n",
    "    lines = file1.readlines()\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = eval(lines[i])\n",
    "\n",
    "    return lines"
   ],
   "metadata": {
    "id": "T76vqJr8icKl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_custom_spacy_model(model_path):\n",
    "    \"\"\"\n",
    "    function to load the custom trained spacy model\n",
    "\n",
    "    input ::\n",
    "        - folder_path : folder which contains model\n",
    "\n",
    "    output ::\n",
    "        - model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading model from {0}\\n\".format(model_path))\n",
    "    nlp = spacy.load(model_path)\n",
    "\n",
    "    return nlp"
   ],
   "metadata": {
    "id": "j2E0yFwnjAfz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_custom_model_accuracy(data):\n",
    "    \"\"\"\n",
    "    function to calculate accuracy of custom trained entity model\n",
    "\n",
    "    input ::\n",
    "        - list containing entity data\n",
    "        \n",
    "    output ::\n",
    "        - accuracy metrics \n",
    "    \"\"\"\n",
    "\n",
    "    nlp = load_custom_spacy_model(\"de_spacy_custom_v2\")\n",
    "\n",
    "    print(\"Calculating score...\")\n",
    "    new_test_data = []\n",
    "\n",
    "    for text, annots in data:\n",
    "        new_test_data.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "    scores_model = nlp.evaluate(new_test_data)\n",
    "\n",
    "    #print scores that you want\n",
    "    precision_model = scores_model[\"ents_p\"]\n",
    "    recall_model = scores_model[\"ents_r\"]\n",
    "    f_score_model = scores_model[\"ents_f\"]\n",
    "    scores_entities = scores_model[\"ents_per_type\"]\n",
    "\n",
    "    print(\"================ Accuracy scores using custom trained model =================\\n\")\n",
    "   \n",
    "    print(\"================= Overall scores =================\\n\")\n",
    "    print(\"Precision : \",precision_model)\n",
    "    print(\"Recall : \",recall_model)\n",
    "    print(\"F1 Score : \",f_score_model)\n",
    "   \n",
    "    print(\"\\n================= Entity wise score =================\\n\")\n",
    "   \n",
    "    print(\"============= Person Entity score =================\\n\")\n",
    "    print(\"Precision : \",scores_entities['PERSON']['p'])\n",
    "    print(\"Recall : \",scores_entities['PERSON']['r'])\n",
    "    print(\"F1 Score : \",scores_entities['PERSON']['r'])\n",
    "\n",
    "    print(\"\\n============= Location Entity score =================\\n\")\n",
    "    print(\"Precision : \",scores_entities['LOC']['p'])\n",
    "    print(\"Recall : \",scores_entities['LOC']['r'])\n",
    "    print(\"F1 Score : \",scores_entities['LOC']['r'])\n",
    "\n",
    "\n",
    "def calculate_pre_trained_model_score(data):\n",
    "    \"\"\"\n",
    "    function to calculate accuracy of custom trained entity model\n",
    "\n",
    "    input ::\n",
    "        - list containing entity data\n",
    "        \n",
    "    output ::\n",
    "        - accuracy metrics \n",
    "    \"\"\"\n",
    "\n",
    "    # using spact large german model\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "    print(\"\\n\\nCalculating score...\")\n",
    "    new_test_data = []\n",
    "\n",
    "    for text, annots in data:\n",
    "        new_test_data.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "\n",
    "    scores_model = nlp.evaluate(new_test_data)\n",
    "\n",
    "    #print scores that you want\n",
    "    precision_model = scores_model[\"ents_p\"]\n",
    "    recall_model = scores_model[\"ents_r\"]\n",
    "    f_score_model = scores_model[\"ents_f\"]\n",
    "    scores_entities = scores_model[\"ents_per_type\"]\n",
    "\n",
    "    print(\"\\n================ Accuracy scores using Pre-trained large model =================\\n\")\n",
    "   \n",
    "    print(\"================= Overall scores =================\\n\")\n",
    "    print(\"Precision : \",precision_model)\n",
    "    print(\"Recall : \",recall_model)\n",
    "    print(\"F1 Score : \",f_score_model)\n",
    "   \n",
    "    print(\"\\n================= Entity wise score =================\\n\")\n",
    "   \n",
    "    print(\"============= Person Entity score =================\\n\")\n",
    "    print(\"Precision : \",scores_entities['PERSON']['p'])\n",
    "    print(\"Recall : \",scores_entities['PERSON']['r'])\n",
    "    print(\"F1 Score : \",scores_entities['PERSON']['r'])\n",
    "\n",
    "    print(\"\\n============= Location Entity score =================\\n\")\n",
    "    print(\"Precision : \",scores_entities['LOC']['p'])\n",
    "    print(\"Recall : \",scores_entities['LOC']['r'])\n",
    "    print(\"F1 Score : \",scores_entities['LOC']['r'])"
   ],
   "metadata": {
    "id": "JA0RD5hRjDI4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#loading data\n",
    "data = load_data()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-yme5IxjGKD",
    "outputId": "a11f7e3f-6362-4a27-93a2-772558795111",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#accuracy score of custom trained model\n",
    "calculate_custom_model_accuracy(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exoMd1uZjKna",
    "outputId": "0dcd1c9b-d11c-4e0a-c498-00c3f0b19027",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#accuracy score of spacy large model\n",
    "calculate_pre_trained_model_score(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hhnh_qjzjQ90",
    "outputId": "1cab9ac6-9d05-4d20-aead-115e0cbdddcb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}